---
title: 'OLS Estimator의 Asymptotic distribution, t-Test'
excerpt: ''
coverImage: ''
date: '2024-02-11T19:13:50.000Z'
categories: [Mathematics, Statistics]
author:
  name: Yongjin
  picture: '/assets/blog/authors/yongjin.png'
ogImage:
  url: ''
alert: ''
---

## OLS estimator의 Limiting distribution

---

2024.02.11

### Multiple Regression

<span style="color:gray">Asymptotic distribution이라는 표현이 더 많이 쓰이는 듯..</span>

$$\textbf{y}=\textbf{X}\beta + e$$ &nbsp; <span style="color:gray"> where $$ \ e \sim N(0, \sigma^2 I)$$ </span> 에서

$$
\begin{aligned}
\hat{\beta} & = (\textbf{X}'\textbf{X})^{-1}\textbf{X}'\textbf{y} \\
& = (\textbf{X}'\textbf{X})^{-1}\textbf{X}'(\textbf{X} \beta + e) \\
& = \beta + (\textbf{X}'\textbf{X})^{-1}\textbf{X}'e \\
\end{aligned}
$$

이므로

- 평균: $$ \mathbb{E}[\hat{\beta} - \beta] = 0 $$ <span style="color:gray"> $$\quad \because \mathbb{E}[e]=0$$ </span>
- 분산: let $$ \ plim (\frac{1}{n}X'X) \rightarrow Q_x, \frac{1}{\sqrt{n}}(x'e) \xrightarrow{\quad d} N(0, \sigma^2 Q_x) $$ .

  $$
  \begin{aligned}
  \sqrt{n}(\hat{\beta} - \beta) = (\frac{1}{n}\textbf{X}'\textbf{X})^{-1}(\frac{1}{\sqrt{n}}\textbf{X}'e) & \xrightarrow{\quad d} N(0, Q_x^{-1} (\sigma^2 Q_x) Q_x^{-1}) \\
  & \xrightarrow{\quad d} N(0, \sigma^2 Q_x^{-1}) \\
  \end{aligned}
  $$

<br>

$$\therefore \sqrt{n}(\hat{\beta} - \beta) \xrightarrow{\quad d} N(0,\sigma^2 Q_x^{-1}) $$

<br><br>

## Standard Error, t-Test, p-value

---

2024.02.11

### Standatd Error

$$ s.e.(T) $$ : estimator $T$ 의 표준편차.

- 결국, **그 estimator의 계산된 값이 얼마나 틀릴 수 있는지에 대한 정보**이다.
- 대표적인 예시가 t-Test에 사용하는 $$s.e.(\hat{\beta}) = \frac{\sigma^2}{X'X}$$ 인데,
- 그런데 $$\sigma^2$$ 는 알 수 없는 값이므로 관측을 통해 구할 수 있는 $$\hat{\sigma}^2 = \frac{1}{n-k}\sum{\hat{e_i}^2} = \frac{1}{n-k-1}\sum{(y_i-\hat{y_i})^2}$$ 을 대신 사용하면 &nbsp; <span style="color:gray">($k$ 는 독립 변수의 개수.)</span>
- <span style="color:#6667ab">$$s.e.(\hat{\beta_j}) = \sqrt{\widehat{Var(\hat{\beta_j})}} = \sqrt{\frac{\hat{\sigma}^2}{[\textbf{X}'\textbf{X}]_{jj}}}$$ </span> ([참고](https://freshrimpsushi.github.io/ko/posts/2464/))
  - <span style="color:gray"> simple regression의 경우에는 $$s.e.(\hat{\beta}) = \sqrt{\widehat{Var(\hat{\beta})}} = \sqrt{\frac{\hat{\sigma}^2}{\sum{(x_i-\bar{x})^2}}}$$ </span>

<br>

### t-Test

#### Two-sample

<span style="color:#6667ab">t-statistic: $$t = \frac{\bar{X_1}-\bar{X_2}}{s_p \cdot \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}, \ s_p = \sqrt{\frac{(n_1-1)S_{X_1}^2 + (n_2-1)S_{X_2}^2}{(n_1-1) + (n_2-1)}}$$ </span> (pooled variance, 두 집단의 분산은 같다고 가정)  
<span style="color:gray"> ($$S_{X_n}$$ 은 표본분산) </span>

- 두 집단의 평균값의 차이가 통계적으로 유의미한지 비교하는 것이 목적
- 두 **평균의 차이**와 **흩어짐의 정도**를 비교.
  - 자료를 모아본 결과 a만큼 흩어짐이 있는데, 두 집단의 평균의 차이가 a정도 난다면 그건 **두 집단이 실제로 차이가 나는 건지 그냥 자료를 모으며 생긴 흩어짐에 불과한 건지 알 수가 없다.**
- 그래서 <U>평균의 차이와 흩어짐의 정도의 비율</U>을 계산한다. **이 값(의 절댓값)이 클수록 t분포의 끝자락에 위치할테니 귀무가설이 기각될 확률이 높아진다.**

  - 귀무가설 $$H_0$$ 는 $$\mu_1 = \mu_2$$ 와 같이 세울 수 있다.
  - 이후 위와 같은 t-statistic 값을 구하고, 상응하는 **t분포**에서 그 값이 얼마나 끝자락에 있는지 확인한다.  
    (모표준편차 $$\sigma$$ 대신 standard error로 나눴기 때문에 t분포를 따른다.)
  - 가령 significance level을 5%로 했고 t-static 값이 1.96보다 크다면, $$H_0$$ 를 기각, 즉 두 표본평균의 차이가 유의미하게 크다는 것으로 생각할 수 있다.
  - 이것이 **t-Test**

- t-statistic은 분모의 분모(=분자)에 $n$ 있는 형태이다. 원래 $n$ 이 작아지면 standard error로가 작아지므로 t-statistic의 값은 커질텐데, 이는 **표본의 개수가 많을수록 t-statistic의 더 확신할 수 있다**는 직관과도 방향이 같다.

  - 이는 아래의 $$s.e.(\hat{\beta})$$ 를 사용하는 경우에도 같다. $$s.e.(\hat{\beta})$$ 의 분모에는 $$\sqrt{n}$$ 이 있고, 그 $$s.e.(\hat{\beta})$$ 는 t-statistic의 분모이므로, 결론적으로 <U>t-statistic의 분자에 $n$ 이 있다</U>. 즉 **$n$ 이 커지면 ($$s.e.(\hat{\beta})$$ 가 작아져서) t-statistic의 값도 커진다.**

- 기타
  - one-tailed test, two-tailed test
  - 독립표본, 대응표본 t-Test
  - 표본 수가 많으면 -> **Z-test**
  - 세 개 이상의 집단의 평균을 비교? -> **ANOVA**

#### One-sample

(2023년 여름학기 계량경제 수업에서 배워온 건 이 내용이었다.)

<span style="color:#6667ab">t-statistic: $$t = \frac{\hat{\beta} - c}{s.e.(\hat{\beta})}$$</span>

- 이는 자유도가 $(n-p-1)$ 인 t-분포 $$t(n-p-1)$$ 을 따른다. ($n$ 이 커질수록 정규분포와 유사해짐)
- 이번엔 두 집단을 비교하는 것이 아니다. $$\hat{\beta}$$ 는 열심히 계산해서 구한 estimate이고, 이를 우리가 비교하고자 하는 값 $c$ 와 비교한다.
  - 만약 $$H_0: \hat{\beta}=0$$ 이라면 $$c=0$$ 이 된다.
  - 위의 두 집단을 비교하는 경우와 다르게 이번에는, 이번에는 한 집단만 평균과 분산을 계산했고, 다른 한 집단은 특정한 평균값에 표본분산이 0인 (=매우 확실히 안다) 상황이라고 생각해볼 수 있겠다.

### p-value

- 그냥 t-Test 개념에서 부산물처럼 이해할 수 있는 개념. (significance level이 5%일 경우로 가정하고)
  1. t-statistic의 값이 1.96보다 큰지 확인할 수도 있지만
  2. t분포에서 $$P(X > t)$$ 의 값이 0.05보다 작은지 확인할 수도 있다. 이 때 계산되는 저 확률값이 **p-value**이다. 결국 1번과 같은 작업을 할 뿐이다.

## References

---

[https://khalidpark2029.tistory.com/34](https://khalidpark2029.tistory.com/34)
[https://angeloyeo.github.io/2020/02/13/Students_t_test.html](https://angeloyeo.github.io/2020/02/13/Students_t_test.html)
[https://bioinformaticsandme.tistory.com/80](https://bioinformaticsandme.tistory.com/80html)
